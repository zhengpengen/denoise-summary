defaults:
  - _self_

seed: 42

# --- Data Loading ---
loader:
  batch_size: 256
  eval_batch_size: 256
  num_workers: 4
  # Batch size for the one-time dataset preprocessing. Larger is often faster.
  preprocessing_batch_size: 1000
  pin_memory: True

# --- Data Settings ---
data:
  dataset_name: ereverter/cnn_dailymail_extractive
  version: null 
  train: train
  validation: validation
  test: test
  cache_dir: ./cache/cnn_dailymail_extractive/

# --- Model Architecture ---
model:
  # for dit
  # dim: 768                # Increased from 512
  # num_layers: 12          # Increased from 8 (Go to 24 for "Large")
  # num_heads: 12           # Increased from 8
  # dim_feedforward: 3072   # Standard 4x scaling (4 * 768)
  # Embedding dimension for sentences and the model's hidden size
  dim: 384
  num_layers: 6
  num_heads: 6
  
  # dim: 512
  # num_layers: 8
  # num_heads: 8
  
  # Dimension of the feedforward network model in nn.TransformerEncoderLayer
  dim_feedforward: 1536 # 4 * dim
  # dim_feedforward: 2048
  
  activation: 'gelu'
  dropout: 0.1

eval:
  checkpoint_path: "/home/michael/denoise-summary/outputs/2025.12.01/082613/best_model.pt"

# --- Task-specific Settings ---
summarization:
  sentence_embedder_name_or_path: 'all-MiniLM-L6-v2'
  sentence_embedding_dim: 384
  # sentence_embedder_name_or_path: 'all-mpnet-base-v2'
  # sentence_embedding_dim: 768
  # Max sentences per document. Documents with more will be truncated.
  max_sentences: 100

# --- Diffusion Process ---
diffusion:
  # Number of noising steps
  # timesteps: 1000 # first one
  # timesteps: 500 # middle ones
  timesteps: 50 # 12/02 080657 onwards
  # Noise schedule parameters
  beta_start: 0.0001
  beta_end: 0.02
  beta_schedule: 'linear' # or 'cosine'

# --- Optimizer ---
optim:
  name: 'AdamW'
  weight_decay: 0.01
  lr: 1e-4
  betas: [0.9, 0.999]
  eps: 1e-8

# --- Trainer Settings ---
trainer:
  accelerator: 'gpu'
  devices: 1
  gradient_clip_val: 1.0
  # Total training steps. Number of epochs will be calculated from this.
  max_steps: 50000
  # Run validation every N training steps
  # Log every N steps
  log_every_n_steps: 100
  # Path to a checkpoint file to resume training from. If null, starts fresh.
  resume_from_checkpoint: null

# --- WandB Logging ---
wandb:
  project: "denoise-summary"
  entity: zhengpengen # Your wandb username or team name
  mode: "online" # "online", "offline", or "disabled"
  notes: "DiT model but decreased size"

# --- Hydra Settings ---
hydra:
  run:
    # dir: /content/drive/MyDrive/_ColabOutputs/cnn_dailymail/${now:%Y.%m.%d}/${now:%H%M%S}
    dir: ./outputs/${now:%Y.%m.%d}/${now:%H%M%S}
  job:
    chdir: true
